{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PPO Base is a minimal, research-faithful implementation of Proximal Policy Optimization (PPO) in PyTorch.**\n",
        "This repository focuses on the core PPO algorithm as described in the original paper, including the clipped surrogate objective, Generalized Advantage Estimation (GAE), and an on-policy actor–critic training loop. The goal is clarity and correctness rather than performance optimizations, making this implementation easy to read, extend, and use as a reference for understanding PPO from first principles"
      ],
      "metadata": {
        "id": "JYqfnYCOI274"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the PPO  require:\n",
        ". actor-critic network\n",
        ". rollout buffer\n",
        ". GAE\n",
        ". PPO clipped objective\n",
        ". update step\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JtBphPtjRso9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "from dataclasses import dataclass\n"
      ],
      "metadata": {
        "id": "hzY3v5Y5QbOz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Policy (Actor) & Value Function (Critic)**\n",
        "What it is\n",
        "\n",
        "Actor: learns what action to take\n",
        "\n",
        "Critic: learns how good a state is\n",
        "\n",
        "PPO is an actor–critic algorithm.\n",
        "\n",
        "In math (paper)\n",
        "\n",
        "**Policy: πθ(a | s)**\n",
        "\n",
        "**Value: Vφ(s)**\n",
        "\n"
      ],
      "metadata": {
        "id": "UOIgHIeAV1aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(state_dim, 64),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.Tanh(),\n",
        "\n",
        "    )\n",
        "\n",
        "    self.policy_head = nn.Linear(64, action_dim)\n",
        "\n",
        "  def forward(self, state):\n",
        "   x = self.net(state)\n",
        "   logits = self.policy_head(x)\n",
        "   dist = Categorical(logits=logits)\n",
        "   return dist\n"
      ],
      "metadata": {
        "id": "AusKVrEfa1e9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_dim):\n",
        "    super().__init__()\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(state_dim, 64),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(64,64),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(64,1)\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, state):\n",
        "     return self.net(state).squeeze(-1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Nh9MtuCc9e1k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a Rollout Buffer?**  \n",
        "A rollout buffer is a temporary storage that collects experiences generated by running the current policy in the environment for a fixed number of steps."
      ],
      "metadata": {
        "id": "xCl3AkrcR4xW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RolloutBuffer:\n",
        "  def __init__(self):\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.logprobs = []\n",
        "    self.values = []\n",
        "    self.rewards = []\n",
        "    self.dones = []\n",
        "\n",
        "\n",
        "def add(self, state, action, logprob, value, reward, done):\n",
        "   self.states.append(state)\n",
        "   self.actions.append(action)\n",
        "   self.logprobs.append(logprob)\n",
        "   self.values.append(value)\n",
        "   self.rewards.append(reward)\n",
        "   self.dones.append(done)\n",
        "\n",
        "\n",
        "def clear(self):\n",
        "  self.states.clear()\n",
        "  self.actions.clear()\n",
        "  self.logprobs.clear()\n",
        "  self.values.clear()\n",
        "  self.rewards.clear()\n",
        "  self.dones.clear()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SHa7EPmwu5He"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAE estimates how much better an action was than expected by combining immediate surprise and future surprise, with controlled decay.\n",
        "\n",
        "It balances:\n",
        "\n",
        "**Bias (being wrong but stable)**\n",
        "\n",
        "**Variance (being correct but noisy)**  "
      ],
      "metadata": {
        "id": "06QfbU7R1few"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gae(rewards, values, dones, last_value, gamma=0.99, lam=0.95):\n",
        "  T = len(rewards)\n",
        "  advantages = torch.zeros(T)\n",
        "  gae = 0\n",
        "  next_value = 0\n",
        "\n",
        "  for t in reversed(range(T)):\n",
        "    delta = rewards[t] + gamma*next_value*(1 - dones[t]) - values[t]\n",
        "    gae = delta + gamma*lam*gae*(1-dones[t])\n",
        "    advantages[t] = gae\n",
        "    next_value = values[t]\n",
        "\n",
        "\n",
        "  return  advantages\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wY5eQD2j2pXp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "$$\n",
        "L^{CLIP}(\\theta) =\n",
        "\\mathbb{E}_t \\left[\n",
        "\\min \\left(\n",
        "r_t(\\theta) A_t,\n",
        "\\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t\n",
        "\\right)\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "r_t(\\theta) =\n",
        "\\frac{\n",
        "\\pi_\\theta(a_t \\mid s_t)\n",
        "}{\n",
        "\\pi_{\\theta_{\\text{old}}}(a_t \\mid s_t)\n",
        "}\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PFkNE85lt6_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def update(self):\n",
        "\n",
        "        # Convert memory to tensors\n",
        "        states = torch.stack(self.states)\n",
        "        actions = torch.tensor(self.actions)\n",
        "        old_logprobs = torch.stack(self.logprobs).detach()\n",
        "\n",
        "        returns = self.compute_returns()\n",
        "\n",
        "        # Get value estimates for advantage\n",
        "        with torch.no_grad():\n",
        "            _, state_values, _ = self.policy.evaluate(states, actions)\n",
        "        advantages = returns - state_values\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            logprobs, state_values, entropy = self.policy.evaluate(states, actions)\n",
        "\n",
        "            ratios = torch.exp(logprobs - old_logprobs)\n",
        "\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.eps_clip,\n",
        "                                         1 + self.eps_clip) * advantages\n",
        "\n",
        "            # Final PPO loss\n",
        "            loss = -torch.min(surr1, surr2) \\\n",
        "                   + 0.5 * self.MseLoss(state_values, returns) \\\n",
        "                   - 0.01 * entropy\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # Clear memory\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n"
      ],
      "metadata": {
        "id": "oI78t0_McXjT"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}